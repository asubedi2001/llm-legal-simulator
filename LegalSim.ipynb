{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8olCj7elnvFN"
      },
      "source": [
        "# Group 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsZd8FOVn29f"
      },
      "source": [
        "Group Members:\n",
        "1.   Chris Devoe\n",
        "2.   Ife Kayode\n",
        "3.   Michael Moore\n",
        "4.   Emmanuel Opoku\n",
        "5.   Aakash Subedi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXZrBYt_mx97"
      },
      "source": [
        "# Get the data from the Legal Stories GitHub repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czwUvq_styuh"
      },
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "#!pip install torch==2.9.0+cu126 torchvision==0.24.0+cu126 torchaudio==2.9.0+cu126 --index-url https://download.pytorch.org/whl/cu126\n",
        "import os\n",
        "#!pip install -q --upgrade torch\n",
        "#!pip install -q transformers triton==3.4 kernels\n",
        "#!pip uninstall -q torchvision torchaudio -y\n",
        "os.environ[\"HF_HOME\"] = \"/content/drive/MyDrive/hf_cache\"\n",
        "os.makedirs(\"/content/drive/MyDrive/hf_cache\", exist_ok=True)\n",
        "\n",
        "\n",
        "# # Get files\n",
        "!wget https://raw.githubusercontent.com/hjian42/LegalStories/main/data/101-doctrines/legal_doctrines_101.tsv\n",
        "!wget https://raw.githubusercontent.com/hjian42/LegalStories/main/data/101-doctrines/gpt3.5_story_question_101.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfapY-1impoH"
      },
      "outputs": [],
      "source": [
        "# Legal Doctrines 101 file\n",
        "import csv\n",
        "def load_doctrines():\n",
        "  # Format: concept\tintro_text\tword_count\n",
        "  with open('legal_doctrines_101.tsv', newline='') as f:\n",
        "    reader = csv.DictReader(f, delimiter='\\t')\n",
        "    data = list(reader)\n",
        "  return data\n",
        "\n",
        "# GPT 3.5 Story Questions 101 file\n",
        "def load_story_questions():\n",
        "  # Format: concept\tintro_text\tstory\tconcept_question\tending_question\tlimitation_question\n",
        "  with open('gpt3.5_story_question_101.tsv', newline='') as f:\n",
        "    reader = csv.DictReader(f, delimiter='\\t')\n",
        "    data = list(reader)\n",
        "  return data\n",
        "\n",
        "doctrines_data = load_doctrines()\n",
        "questions_data = load_story_questions()\n",
        "\n",
        "print(\"Doctrines Data: \")\n",
        "for row in doctrines_data:\n",
        "  print(row)\n",
        "\n",
        "print(\"\\nStory Questions Data: \")\n",
        "for row in questions_data:\n",
        "  print(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data for Finetuning"
      ],
      "metadata": {
        "id": "gsVXL-qVOQwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "\n",
        "def get_question_and_concept(question):\n",
        "  prompt =\"\"\n",
        "  completion = \"\"\n",
        "  prompt += \"Concept: \"\n",
        "  prompt += question[\"concept\"]\n",
        "  prompt += \". Summary Text: \"\n",
        "  prompt += question[\"intro_text\"]\n",
        "\n",
        "  completion += \"Story: \"\n",
        "  completion += question[\"story\"]\n",
        "  # May not need concept question for what we want to do, can be added if needed\n",
        "  # completion += \" Concept Question: \"\n",
        "  # completion += question[\"concept_question\"]\n",
        "  # completion += \" Limitation Question: \"\n",
        "  # completion += question[\"limitation_question\"]\n",
        "  completion += \" Question: \"\n",
        "  completion += question[\"ending_question\"]\n",
        "  completion_new = completion.replace(\"\\n\\n\", \"\\n\")\n",
        "  return prompt, completion_new\n",
        "\n",
        "\n",
        "def prepare_questions_finetuning_data(raw_data, filename='finetuning_data_questions.jsonl'):\n",
        "  finetuning_data = []\n",
        "  for question in raw_data:\n",
        "    data = {}\n",
        "    prompt, completion = get_question_and_concept(question)\n",
        "    data[\"messages\"] = [{\"role\":\"user\", \"content\":prompt}, {\"role\":\"assistant\", \"content\":completion}]\n",
        "    finetuning_data.append(data)\n",
        "  with open(filename, 'w') as out:\n",
        "    for data in finetuning_data:\n",
        "      out.write(json.dumps(data))\n",
        "      out.write('\\n')\n",
        "\n",
        "prepare_questions_finetuning_data(questions_data)\n"
      ],
      "metadata": {
        "id": "irdeFZMIOh8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print head of file\n",
        "!head finetuning_data_questions.jsonl"
      ],
      "metadata": {
        "id": "jaBhY2krO7Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune GPT3.5"
      ],
      "metadata": {
        "id": "cHzPzuMFxsBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import openai\n",
        "\n",
        "print(\"Enter openai api key: \")\n",
        "openai.api_key = input()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=openai.api_key"
      ],
      "metadata": {
        "id": "quFhcg_jxzg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI()\n",
        "\n",
        "client.files.create(\n",
        "    file=open(\"finetuning_data_questions.jsonl\",'rb'),\n",
        "    purpose=\"fine-tune\"\n",
        ")"
      ],
      "metadata": {
        "id": "NFuzzz7vyNCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training File ID:')\n",
        "training_file = input()"
      ],
      "metadata": {
        "id": "Lz_8Z540yx6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.create(\n",
        "  training_file=training_file,\n",
        "  model=\"gpt-3.5-turbo-0125\"\n",
        ")"
      ],
      "metadata": {
        "id": "1JQjmQjtzF_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a random topic for the case\n",
        "import random\n",
        "topic = random.choice(doctrines_data)\n",
        "\n",
        "# Create examples for the prompt\n",
        "ex1in, ex1out = get_question_and_concept(questions_data[0])\n",
        "ex2in, ex2out = get_question_and_concept(questions_data[1])\n",
        "ex3in, ex3out = get_question_and_concept(questions_data[2])\n",
        "story_so_far = None # Used to pass the previous question to prompt\n",
        "\n",
        "# Create the prompt\n",
        "# Used Chatgpt to improve the prompt to make the story continuous\n",
        "def initial_prompt():\n",
        "  return [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are a legal case question generator that creates a multi-step case based on legal concepts. \"\n",
        "            \"You will produce a multiple-choice question built around a short narrative introduction. \"\n",
        "            \"The story must introduce characters, a location, and a situation that can be expanded later.\"\n",
        "            \"You MUST write in this structure:\\n\\n\"\n",
        "            \"1. STORY: A 3–6 sentence narrative introducing the situation.\\n\"\n",
        "            \"2. QUESTION: A single legal question about the concept.\\n\"\n",
        "            \"3. /ANSWER: The correct choice.\\n\"\n",
        "            \"4. RATIONALE: A short explanation.\\n\\n\"\n",
        "            \"Do NOT end the story — leave open threads so the next question can continue it.\\n\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            f\"Concept: {ex1in}\\nQuestion: {ex1out}\\n\"\n",
        "            f\"Concept: {ex2in}\\nQuestion: {ex2out}\\n\"\n",
        "            f\"Concept: {ex3in}\\nQuestion: {ex3out}\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Concept: {topic['concept']}. Summary Text: {topic['intro_text']}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def middle_prompt(story_so_far):\n",
        "  return [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are continuing a legal case story. Your job is to write the NEXT question in the same narrative. \"\n",
        "            \"You MUST continue the same characters, same setting, same timeline, and the same story threads. \"\n",
        "            \"Use the previous story as canon and extend it naturally.\\n\\n\"\n",
        "            \"Your structure MUST be:\\n\"\n",
        "            \"1. STORY: 3–6 new sentences that continue the narrative.\\n\"\n",
        "            \"2. QUESTION: A new multiple-choice question about a NEW legal concept.\\n\"\n",
        "            \"3. /ANSWER.\\n\"\n",
        "            \"4. RATIONALE.\\n\\n\"\n",
        "            \"Do not contradict earlier facts. Do not end the story — keep it open.\\n\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            f\"Concept: {ex1in}\\nQuestion: {ex1out}\\n\"\n",
        "            f\"Concept: {ex2in}\\nQuestion: {ex2out}\\n\"\n",
        "            f\"Concept: {ex3in}\\nQuestion: {ex3out}\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Story so far:\\n{story_so_far}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def end_prompt(story_so_far):\n",
        "  return [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"Write the final question in this legal case. Continue the story from the previous section, \"\n",
        "            \"but this time bring the situation to a conclusion.\\n\\n\"\n",
        "            \"Structure:\\n\"\n",
        "            \"1. STORY: 3–6 sentences resolving the conflict.\\n\"\n",
        "            \"2. QUESTION: A final legal concept question.\\n\"\n",
        "            \"3. /ANSWER.\\n\"\n",
        "            \"4. RATIONALE.\\n\\n\"\n",
        "            \"The story should conclude all major plot threads.\\n\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            f\"Concept: {ex1in}\\nQuestion: {ex1out}\\n\"\n",
        "            f\"Concept: {ex2in}\\nQuestion: {ex2out}\\n\"\n",
        "            f\"Concept: {ex3in}\\nQuestion: {ex3out}\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Story so far:\\n{story_so_far}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def generate_response(prompt, finetuned_model):\n",
        "  response = client.chat.completions.create(\n",
        "    model=finetuned_model,\n",
        "    messages=prompt,\n",
        "    temperature=0.7,\n",
        "    max_tokens=500,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    stop=[\"###\"]\n",
        "    )\n",
        "  return response.choices[0].message.content.strip()\n",
        "\n",
        "question = generate_response(initial_prompt(), \"gpt-3.5-turbo-0125\")\n",
        "print(\"BEGINNING: \")\n",
        "print(question)\n",
        "print(\"\\n\\n\")\n",
        "story_so_far = question.split(\"/\")[0]\n",
        "question2 = generate_response(middle_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "print(\"MIDDLE: \")\n",
        "print(question2)\n",
        "print(\"\\n\\n\")\n",
        "story_so_far += question2.split(\"/\")[0]\n",
        "question3 = generate_response(end_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "print(\"END: \")\n",
        "print(question3)"
      ],
      "metadata": {
        "id": "zuQSl48R4W3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Reasoning Model"
      ],
      "metadata": {
        "id": "VD8Luo9jsSJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"What is your response? \")\n",
        "user_response = \"The answer is A\""
      ],
      "metadata": {
        "id": "xpBI6yI9gAce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using o3-mini, GptOss was way too big for colab to handle\n",
        "from openai import OpenAI\n",
        "client2 = OpenAI()\n",
        "\n",
        "o3mini_prompt = (\n",
        "    \"You are a legal evaluator. Given a question, the correct response, and the users response\"\n",
        "    \"return a score out of 4 points. \"\n",
        "    \"1 point for the correct letter answer.\"\n",
        "    \"1 point for mentioning the right doctrines/concept/keywords.\"\n",
        "    \"1 point for an okay justification or 2 points for a good justification.\"\n",
        "    \"Provide your resoning for the score. An example of how it should be formatted is below: \"\n",
        "    \"Example: \"\n",
        "    \"Score: 4\\n\"\n",
        "    \"Rationale: You provided the correct answer to the question, and mentioned the concept of legality.\"\n",
        "    \"Your justification could use some improvements though, it seemed somewhat ambiguous.\"\n",
        ")\n",
        "\n",
        "def evaluate_response(question, user_response, prompt, model):\n",
        "  response = client2.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {question}\\nUser Response: {user_response}\"}\n",
        "    ]\n",
        "  )\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")"
      ],
      "metadata": {
        "id": "U40qzgAJsd_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dna8Iv356bOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo Game"
      ],
      "metadata": {
        "id": "lpMYrwLjfwdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions_to_win = 6\n",
        "story_so_far = None\n",
        "user_response = \"\"\n",
        "win = False\n",
        "lose = False\n",
        "count = 0\n",
        "\n",
        "# Run game (use -1 to quit for now)\n",
        "while not win and not lose and user_response != \"-1\":\n",
        "  if count == 0:\n",
        "    # Start case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(initial_prompt(), \"gpt-3.5-turbo-0125\")\n",
        "    print(question)\n",
        "    story_so_far = question.split(\"/\")[0]\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    print(evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\"))\n",
        "  elif count == questions_to_win:\n",
        "    # End case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(end_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "    print(question)\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    print(evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\"))\n",
        "  else:\n",
        "    # Middle case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(middle_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "    print(question)\n",
        "    story_so_far += question.split(\"/\")[0]\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    print(evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\"))\n",
        "print(\"CASE CLOSED.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OTgj39VC7HAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## test with scoring\n",
        "def get_score(model_response):\n",
        "  score = model_response.split(\"\\n\")[0]\n",
        "  score = int(score.split(\" \")[1])\n",
        "  return score\n",
        "\n",
        "def get_letter_grade(grade):\n",
        "  letter_grade = \"\"\n",
        "\n",
        "  if 100 >= grade >= 90:\n",
        "    letter_grade = \"A\"\n",
        "  elif 89 >= grade >= 80:\n",
        "    letter_grade = \"B\"\n",
        "  elif 79 >= grade >= 70:\n",
        "    letter_grade = \"C\"\n",
        "  elif 69 >= grade >= 60:\n",
        "    letter_grade = \"D\"\n",
        "  elif 59 >= grade >= 0:\n",
        "    letter_grade = \"F\"\n",
        "  else:\n",
        "    letter_grade = \"error\"\n",
        "\n",
        "  return letter_grade\n",
        "\n",
        "questions_to_win = 6\n",
        "story_so_far = None\n",
        "user_response = \"\"\n",
        "win = False\n",
        "stop = False\n",
        "total_points = 0\n",
        "count = 0\n",
        "\n",
        "# Run game (use -1 to quit for now)\n",
        "while not stop and user_response != \"-1\":\n",
        "  if count == 0:\n",
        "    # Start case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(initial_prompt(), \"gpt-3.5-turbo-0125\")\n",
        "    print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "    story_so_far = question.split(\"/\")[0]\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "    print(model_response)\n",
        "    total_points += get_score(model_response)\n",
        "\n",
        "  elif count == questions_to_win - 1:\n",
        "    # End case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(end_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "    print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "    print(model_response)\n",
        "    total_points += get_score(model_response)\n",
        "    stop = True\n",
        "\n",
        "  else:\n",
        "    # Middle case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(middle_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "    print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "    story_so_far += question.split(\"/\")[0]\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "    print(model_response)\n",
        "    total_points += get_score(model_response)\n",
        "\n",
        "  count += 1\n",
        "print(\"CASE CLOSED.\\n\")\n",
        "\n",
        "print(\"Results:\")\n",
        "grade = (total_points / (questions_to_win * 4)) * 100\n",
        "letter_grade = get_letter_grade(grade)\n",
        "\n",
        "if grade >= 70:\n",
        "  win = True\n",
        "\n",
        "if win:\n",
        "  print(letter_grade, \"\\nGrade: \" + str(grade) + \"%\\nYou've Won!\")\n",
        "else:\n",
        "  print(letter_grade, \"\\nGrade: \" + str(grade) + \"%\\nYou've Lost.\")"
      ],
      "metadata": {
        "id": "1dxnA12GcbTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWNgWoEh_cJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjDczT9o_f_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Demo Game With Scoring and Hints**"
      ],
      "metadata": {
        "id": "ST8FLBvpWRFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "o3mini_hprompt =  (\n",
        "    \"You are a legal evaluator. Given a question, the correct response, a previous hint already given\"\n",
        "    \"return a hint (no need to preface with anything, just the text) to help get the correct answer.\"\n",
        "    \"The hint should be concise and relevant, and should be more helpful than the previous hint\"\n",
        ")"
      ],
      "metadata": {
        "id": "p2-CMW7F_gCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Scoring with hints\n",
        "def get_score(model_response):\n",
        "  score = model_response.split(\"\\n\")[0]\n",
        "  score = int(score.split(\" \")[1])\n",
        "  return score\n",
        "\n",
        "def get_letter_grade(grade):\n",
        "  letter_grade = \"\"\n",
        "\n",
        "  if 100 >= grade >= 90:\n",
        "    letter_grade = \"A\"\n",
        "  elif 89 >= grade >= 80:\n",
        "    letter_grade = \"B\"\n",
        "  elif 79 >= grade >= 70:\n",
        "    letter_grade = \"C\"\n",
        "  elif 69 >= grade >= 60:\n",
        "    letter_grade = \"D\"\n",
        "  elif 59 >= grade >= 0:\n",
        "    letter_grade = \"F\"\n",
        "  else:\n",
        "    letter_grade = \"error\"\n",
        "\n",
        "  return letter_grade\n",
        "\n",
        "\n",
        "def demo():\n",
        "  questions_to_win = 6\n",
        "  story_so_far = None\n",
        "  user_response = \"\"\n",
        "  win = False\n",
        "  stop = False\n",
        "  total_points = 0\n",
        "  count = 0\n",
        "  amount_hint = 0\n",
        "  hint = \"\"\n",
        "\n",
        "  print(\"Welcome to Legal Simulator!\\nYou will be asked 6 questions in total. The highest score per question is 4.\\nYou can use up to 3 hints per question by entering 'Hint'.\\nGet a 70% overall to win the case, good luck!\")\n",
        "  # Run game (use -1 to quit for now)\n",
        "  while not stop and user_response != \"-1\":\n",
        "    if count == 0:\n",
        "      # Start case\n",
        "      amount_hint = 0\n",
        "      print(\"\\nJUDGE: \")\n",
        "      question = generate_response(initial_prompt(), \"gpt-3.5-turbo-0125\")\n",
        "      print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "      story_so_far = question.split(\"/\")[0]\n",
        "\n",
        "      while True:\n",
        "        print(\"\\nATTORNEY (Enter response): \")\n",
        "        user_response = input()\n",
        "        if (user_response == \"hint\" or user_response == \"Hint\") and amount_hint <= 2:\n",
        "          hint = evaluate_response(question, hint, o3mini_hprompt, \"o3-mini\")\n",
        "          print(\"\\nHINT:\")\n",
        "          print(hint)\n",
        "          amount_hint += 1\n",
        "        elif (user_response == \"hint\" or user_response == \"Hint\") and amount_hint > 2:\n",
        "          print(\"\\nYOU HAVE USED UP THE MAXIMUM AMOUNT OF HINTS (3):\")\n",
        "        else:\n",
        "          break\n",
        "\n",
        "      print(\"\\nJURY: \")\n",
        "      model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "      print(model_response)\n",
        "      total_points += get_score(model_response)\n",
        "\n",
        "    elif count == questions_to_win - 1:\n",
        "      # End case\n",
        "      amount_hint = 0\n",
        "      print(\"\\nJUDGE: \")\n",
        "      question = generate_response(end_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "      print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "\n",
        "      while True:\n",
        "        print(\"\\nATTORNEY (Enter response): \")\n",
        "        user_response = input()\n",
        "        if (user_response == \"hint\" or user_response == \"Hint\") and amount_hint <= 2:\n",
        "          hint = evaluate_response(question, hint, o3mini_hprompt, \"o3-mini\")\n",
        "          print(\"\\nHINT:\")\n",
        "          print(hint)\n",
        "          amount_hint += 1\n",
        "        elif (user_response == \"hint\" or user_response == \"Hint\") and amount_hint > 2:\n",
        "          print(\"\\nYOU HAVE USED UP THE MAXIMUM AMOUNT OF HINTS (3):\")\n",
        "        else:\n",
        "          break\n",
        "\n",
        "      print(\"\\nJURY: \")\n",
        "      model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "      print(model_response)\n",
        "      total_points += get_score(model_response)\n",
        "      stop = True\n",
        "\n",
        "    else:\n",
        "      # Middle case\n",
        "      amount_hint = 0\n",
        "      print(\"\\nJUDGE: \")\n",
        "      question = generate_response(middle_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "      print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "      story_so_far += question.split(\"/\")[0]\n",
        "\n",
        "      while True:\n",
        "        print(\"\\nATTORNEY (Enter response): \")\n",
        "        user_response = input()\n",
        "        if (user_response == \"hint\" or user_response == \"Hint\") and amount_hint <= 2:\n",
        "          hint = evaluate_response(question, hint, o3mini_hprompt, \"o3-mini\")\n",
        "          print(\"\\nHINT:\")\n",
        "          print(hint)\n",
        "          amount_hint += 1\n",
        "        elif (user_response == \"hint\" or user_response == \"Hint\") and amount_hint > 2:\n",
        "          print(\"\\nYOU HAVE USED UP THE MAXIMUM AMOUNT OF HINTS (3):\")\n",
        "        else:\n",
        "          break\n",
        "      print(\"\\nJURY: \")\n",
        "\n",
        "      model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "      print(model_response)\n",
        "      total_points += get_score(model_response)\n",
        "\n",
        "    count += 1\n",
        "  print(\"CASE CLOSED.\\n\")\n",
        "\n",
        "  print(\"Results:\")\n",
        "  grade = (total_points / (questions_to_win * 4)) * 100\n",
        "  letter_grade = get_letter_grade(grade)\n",
        "\n",
        "  if grade >= 70:\n",
        "    win = True\n",
        "\n",
        "  if win:\n",
        "    print(\"You got a\", letter_grade, \"on this case!\\nGrade: \" + str(grade) + \"%\\nYou've Won!\")\n",
        "  else:\n",
        "    print(\"You got a\", letter_grade, \"on this case :(\\nGrade: \" + str(grade) + \"%\\nYou've Lost.\")"
      ],
      "metadata": {
        "id": "ktXtM5hf_c36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Play the Demo:**"
      ],
      "metadata": {
        "id": "qe1AZvUM3xxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# play the game\n",
        "demo()"
      ],
      "metadata": {
        "id": "dABC2EKAdIAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Put any information here that we may need later (EX: Finetuning ID)\n",
        "*   finetuning file ID: file-GHFiaB4k8MKRKnDKg4DXde\n",
        "*   finetuning job ID: ftjob-4Hx4CLHnu3bTaKEoecA2roXm"
      ],
      "metadata": {
        "id": "VsLFcdgCy6Io"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}