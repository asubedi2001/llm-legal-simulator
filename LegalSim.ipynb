{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8olCj7elnvFN"
      },
      "source": [
        "# Group 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsZd8FOVn29f"
      },
      "source": [
        "Group Members:\n",
        "1.   Chris Devoe\n",
        "2.   Ife Kayode\n",
        "3.   Michael Moore\n",
        "4.   Emmanuel Opoku\n",
        "5.   Aakash Subedi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXZrBYt_mx97"
      },
      "source": [
        "# Get the data from the Legal Stories GitHub repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czwUvq_styuh",
        "outputId": "0ad1f72e-6060-42d9-bb68-cfddc3ffe827"
      },
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "#!pip install torch==2.9.0+cu126 torchvision==0.24.0+cu126 torchaudio==2.9.0+cu126 --index-url https://download.pytorch.org/whl/cu126\n",
        "import os\n",
        "#!pip install -q --upgrade torch\n",
        "#!pip install -q transformers triton==3.4 kernels\n",
        "#!pip uninstall -q torchvision torchaudio -y\n",
        "os.environ[\"HF_HOME\"] = \"/content/drive/MyDrive/hf_cache\"\n",
        "os.makedirs(\"/content/drive/MyDrive/hf_cache\", exist_ok=True)\n",
        "\n",
        "\n",
        "# # Get files\n",
        "!wget https://raw.githubusercontent.com/hjian42/LegalStories/main/data/101-doctrines/legal_doctrines_101.tsv\n",
        "!wget https://raw.githubusercontent.com/hjian42/LegalStories/main/data/101-doctrines/gpt3.5_story_question_101.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfapY-1impoH",
        "outputId": "1d9d242f-b8d4-439d-9004-bf3f39474d87"
      },
      "outputs": [],
      "source": [
        "# Legal Doctrines 101 file\n",
        "import csv\n",
        "def load_doctrines():\n",
        "  # Format: concept\tintro_text\tword_count\n",
        "  with open('legal_doctrines_101.tsv', newline='') as f:\n",
        "    reader = csv.DictReader(f, delimiter='\\t')\n",
        "    data = list(reader)\n",
        "  return data\n",
        "\n",
        "# GPT 3.5 Story Questions 101 file\n",
        "def load_story_questions():\n",
        "  # Format: concept\tintro_text\tstory\tconcept_question\tending_question\tlimitation_question\n",
        "  with open('gpt3.5_story_question_101.tsv', newline='') as f:\n",
        "    reader = csv.DictReader(f, delimiter='\\t')\n",
        "    data = list(reader)\n",
        "  return data\n",
        "\n",
        "doctrines_data = load_doctrines()\n",
        "questions_data = load_story_questions()\n",
        "\n",
        "print(\"Doctrines Data: \")\n",
        "for row in doctrines_data:\n",
        "  print(row)\n",
        "\n",
        "print(\"\\nStory Questions Data: \")\n",
        "for row in questions_data:\n",
        "  print(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsVXL-qVOQwr"
      },
      "source": [
        "# Prepare Data for Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irdeFZMIOh8I"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "\n",
        "def get_question_and_concept(question):\n",
        "  prompt =\"\"\n",
        "  completion = \"\"\n",
        "  prompt += \"Concept: \"\n",
        "  prompt += question[\"concept\"]\n",
        "  prompt += \". Summary Text: \"\n",
        "  prompt += question[\"intro_text\"]\n",
        "\n",
        "  completion += \"Story: \"\n",
        "  completion += question[\"story\"]\n",
        "  # May not need concept question for what we want to do, can be added if needed\n",
        "  # completion += \" Concept Question: \"\n",
        "  # completion += question[\"concept_question\"]\n",
        "  # completion += \" Limitation Question: \"\n",
        "  # completion += question[\"limitation_question\"]\n",
        "  completion += \" Question: \"\n",
        "  completion += question[\"ending_question\"]\n",
        "  completion_new = completion.replace(\"\\n\\n\", \"\\n\")\n",
        "  return prompt, completion_new\n",
        "\n",
        "\n",
        "def prepare_questions_finetuning_data(raw_data, filename='finetuning_data_questions.jsonl'):\n",
        "  finetuning_data = []\n",
        "  for question in raw_data:\n",
        "    data = {}\n",
        "    prompt, completion = get_question_and_concept(question)\n",
        "    data[\"messages\"] = [{\"role\":\"user\", \"content\":prompt}, {\"role\":\"assistant\", \"content\":completion}]\n",
        "    finetuning_data.append(data)\n",
        "  with open(filename, 'w') as out:\n",
        "    for data in finetuning_data:\n",
        "      out.write(json.dumps(data))\n",
        "      out.write('\\n')\n",
        "\n",
        "prepare_questions_finetuning_data(questions_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaBhY2krO7Kk",
        "outputId": "44ed0cc7-99e3-4f03-ea75-4c34c2fcd35a"
      },
      "outputs": [],
      "source": [
        "# Print head of file\n",
        "!head finetuning_data_questions.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHzPzuMFxsBd"
      },
      "source": [
        "# Finetune GPT3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quFhcg_jxzg7",
        "outputId": "b95b5410-8e17-46c5-eadd-d159e4bca42e"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "import openai\n",
        "\n",
        "print(\"Enter openai api key: \")\n",
        "openai.api_key = input()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=openai.api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFuzzz7vyNCu",
        "outputId": "65d906f2-072d-4dcc-fcf3-365ecd1f1df8"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI()\n",
        "\n",
        "client.files.create(\n",
        "    file=open(\"finetuning_data_questions.jsonl\",'rb'),\n",
        "    purpose=\"fine-tune\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz_8Z540yx6S",
        "outputId": "7874a618-8af8-4903-afc7-7b2db9957560"
      },
      "outputs": [],
      "source": [
        "print('Training File ID:')\n",
        "training_file = input()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JQjmQjtzF_o",
        "outputId": "12cc281a-8c50-4a1c-a7f0-faf9942cafa7"
      },
      "outputs": [],
      "source": [
        "client.fine_tuning.jobs.create(\n",
        "  training_file=training_file,\n",
        "  model=\"gpt-3.5-turbo-0125\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuQSl48R4W3c",
        "outputId": "d6b98cb0-d684-4698-ce40-bc0f9d79dadf"
      },
      "outputs": [],
      "source": [
        "# Choose a random topic for the case\n",
        "import random\n",
        "topic = random.choice(doctrines_data)\n",
        "\n",
        "# Create examples for the prompt\n",
        "ex1in, ex1out = get_question_and_concept(questions_data[0])\n",
        "ex2in, ex2out = get_question_and_concept(questions_data[1])\n",
        "ex3in, ex3out = get_question_and_concept(questions_data[2])\n",
        "story_so_far = None # Used to pass the previous question to prompt\n",
        "\n",
        "# Create the prompt\n",
        "# Used Chatgpt to improve the prompt to make the story continuous\n",
        "def initial_prompt():\n",
        "  return [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are a legal case question generator that creates a multi-step case based on legal concepts. \"\n",
        "            \"You will produce a multiple-choice question built around a short narrative introduction. \"\n",
        "            \"The story must introduce characters, a location, and a situation that can be expanded later.\"\n",
        "            \"You MUST write in this structure:\\n\\n\"\n",
        "            \"1. STORY: A 3–6 sentence narrative introducing the situation.\\n\"\n",
        "            \"2. QUESTION: A single legal question about the concept.\\n\"\n",
        "            \"3. /ANSWER: The correct choice.\\n\"\n",
        "            \"4. RATIONALE: A short explanation.\\n\\n\"\n",
        "            \"Do NOT end the story — leave open threads so the next question can continue it.\\n\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            f\"Concept: {ex1in}\\nQuestion: {ex1out}\\n\"\n",
        "            f\"Concept: {ex2in}\\nQuestion: {ex2out}\\n\"\n",
        "            f\"Concept: {ex3in}\\nQuestion: {ex3out}\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Concept: {topic['concept']}. Summary Text: {topic['intro_text']}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def middle_prompt(story_so_far):\n",
        "  return [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are continuing a legal case story. Your job is to write the NEXT question in the same narrative. \"\n",
        "            \"You MUST continue the same characters, same setting, same timeline, and the same story threads. \"\n",
        "            \"Use the previous story as canon and extend it naturally.\\n\\n\"\n",
        "            \"Your structure MUST be:\\n\"\n",
        "            \"1. STORY: 3–6 new sentences that continue the narrative.\\n\"\n",
        "            \"2. QUESTION: A new multiple-choice question about a NEW legal concept.\\n\"\n",
        "            \"3. /ANSWER.\\n\"\n",
        "            \"4. RATIONALE.\\n\\n\"\n",
        "            \"Do not contradict earlier facts. Do not end the story — keep it open.\\n\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            f\"Concept: {ex1in}\\nQuestion: {ex1out}\\n\"\n",
        "            f\"Concept: {ex2in}\\nQuestion: {ex2out}\\n\"\n",
        "            f\"Concept: {ex3in}\\nQuestion: {ex3out}\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Story so far:\\n{story_so_far}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def end_prompt(story_so_far):\n",
        "  return [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"Write the final question in this legal case. Continue the story from the previous section, \"\n",
        "            \"but this time bring the situation to a conclusion.\\n\\n\"\n",
        "            \"Structure:\\n\"\n",
        "            \"1. STORY: 3–6 sentences resolving the conflict.\\n\"\n",
        "            \"2. QUESTION: A final legal concept question.\\n\"\n",
        "            \"3. /ANSWER.\\n\"\n",
        "            \"4. RATIONALE.\\n\\n\"\n",
        "            \"The story should conclude all major plot threads.\\n\\n\"\n",
        "            \"Examples:\\n\"\n",
        "            f\"Concept: {ex1in}\\nQuestion: {ex1out}\\n\"\n",
        "            f\"Concept: {ex2in}\\nQuestion: {ex2out}\\n\"\n",
        "            f\"Concept: {ex3in}\\nQuestion: {ex3out}\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Story so far:\\n{story_so_far}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def generate_response(prompt, finetuned_model):\n",
        "  response = client.chat.completions.create(\n",
        "    model=finetuned_model,\n",
        "    messages=prompt,\n",
        "    temperature=0.7,\n",
        "    max_tokens=500,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    stop=[\"###\"]\n",
        "    )\n",
        "  return response.choices[0].message.content.strip()\n",
        "\n",
        "question = generate_response(initial_prompt(), \"gpt-3.5-turbo-0125\")\n",
        "print(\"BEGINNING: \")\n",
        "print(question)\n",
        "print(\"\\n\\n\")\n",
        "story_so_far = question.split(\"/\")[0]\n",
        "question2 = generate_response(middle_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "print(\"MIDDLE: \")\n",
        "print(question2)\n",
        "print(\"\\n\\n\")\n",
        "story_so_far += question2.split(\"/\")[0]\n",
        "question3 = generate_response(end_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "print(\"END: \")\n",
        "print(question3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD8Luo9jsSJV"
      },
      "source": [
        "# Setup Reasoning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpBI6yI9gAce",
        "outputId": "809f3815-3d7f-46d6-bf61-1cdb03717817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is your response? \n",
            "B\n"
          ]
        }
      ],
      "source": [
        "print(\"What is your response? \")\n",
        "user_response = input()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "U40qzgAJsd_R",
        "outputId": "b439f794-e7bd-4bb1-e6b6-005a1caff537"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Score: 1\\nRationale: You provided the correct letter answer (\"B\"), which secures 1 point. However, you did not mention any relevant doctrines, concepts, or keywords (such as \"equal authenticity\" or \"bilingual laws\") and you did not provide any justification for your answer.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using o3-mini, GptOss was way too big for colab to handle\n",
        "from openai import OpenAI\n",
        "client2 = OpenAI()\n",
        "\n",
        "o3mini_prompt = (\n",
        "    \"You are a legal evaluator. Given a question, the correct response, and the users response\"\n",
        "    \"return a score out of 4 points. \"\n",
        "    \"1 point for the correct letter answer.\"\n",
        "    \"1 point for mentioning the right doctrines/concept/keywords.\"\n",
        "    \"1 point for an okay justification or 2 points for a good justification.\"\n",
        "    \"Provide your resoning for the score. An example of how it should be formatted is below: \"\n",
        "    \"Example: \"\n",
        "    \"Score: 4\\n\"\n",
        "    \"Rationale: You provided the correct answer to the question, and mentioned the concept of legality.\"\n",
        "    \"Your justification could use some improvements though, it seemed somewhat ambiguous.\"\n",
        ")\n",
        "\n",
        "def evaluate_response(question, user_response, prompt, model):\n",
        "  response = client2.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {question}\\nUser Response: {user_response}\"}\n",
        "    ]\n",
        "  )\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpMYrwLjfwdm"
      },
      "source": [
        "# Demo Game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTgj39VC7HAF"
      },
      "outputs": [],
      "source": [
        "questions_to_win = 6\n",
        "story_so_far = None\n",
        "user_response = \"\"\n",
        "win = False\n",
        "lose = False\n",
        "count = 0\n",
        "\n",
        "# Run game (use -1 to quit for now)\n",
        "while not win and not lose and user_response != \"-1\":\n",
        "  if count == 0:\n",
        "    # Start case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(initial_prompt(), \"gpt-3.5-turbo-0125\")\n",
        "    print(question)\n",
        "    story_so_far = question.split(\"/\")[0]\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    print(evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\"))\n",
        "  elif count == questions_to_win:\n",
        "    # End case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(end_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "    print(question)\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    print(evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\"))\n",
        "  else:\n",
        "    # Middle case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(middle_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "    print(question)\n",
        "    story_so_far += question.split(\"/\")[0]\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    print(evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\"))\n",
        "print(\"CASE CLOSED.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dxnA12GcbTJ",
        "outputId": "b4f52a53-a3c9-452c-de88-9fe83c976cf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "JUDGE: \n",
            "Story: Sarah, a thrill-seeker, decides to go bungee jumping off a bridge during her vacation in New Zealand. Before taking the leap, she signs a waiver acknowledging the risks involved in the activity, including the potential for injury or death. Sarah is well aware of the dangers associated with bungee jumping but is excited about the adrenaline rush. As she plunges towards the water, the bungee cord snaps, causing her to sustain injuries upon impact. Despite her injuries, Sarah cannot hold the bungee jumping company liable under the principle of volenti non fit iniuria.\n",
            "\n",
            "Question: In the scenario described, why is Sarah unable to bring a claim against the bungee jumping company for her injuries?\n",
            "(A) Sarah did not sign a waiver before bungee jumping.\n",
            "(B) Sarah was forced to go bungee jumping against her will.\n",
            "(C) Sarah voluntarily assumed the risks associated with bungee jumping.\n",
            "(D) Sarah's injuries were caused by a third party unrelated to the bungee jumping company.\n",
            "\n",
            "ATTORNEY (Enter response): \n",
            "The correct answer is C because nobody forced her to do it\n",
            "\n",
            "JURY: \n",
            "Score: 3  \n",
            "Rationale: You correctly selected answer (C) and indicated that Sarah voluntarily assumed the risks, which is the core doctrine behind volenti non fit iniuria. However, while your explanation (“nobody forced her to do it”) captures the basic idea of voluntary risk assumption, it lacks a robust reference to the legal principle or additional details (such as the significance of the waiver in this context). This justification is acceptable but would benefit from further elaboration to reach the highest score.\n",
            "\n",
            "JUDGE: \n",
            "Story: Sarah, after her bungee jumping incident, is now recovering from her injuries back in her home country. She contemplates her next steps regarding the situation and the legal aspects surrounding it. Despite the physical pain she is in, Sarah reflects on the waiver she signed before the jump and the risks she willingly accepted as an adventure enthusiast.\n",
            "\n",
            "Question: Considering Sarah's situation and the waiver she signed before bungee jumping, what legal doctrine is likely to protect the bungee jumping company from liability in this scenario?\n",
            "(A) Res ipsa loquitur.\n",
            "(B) Contributory negligence.\n",
            "(C) Assumption of risk.\n",
            "(D) Negligence per se.\n",
            "\n",
            "ATTORNEY (Enter response): \n",
            "The correct answer is C because it is a dangerous activity\n",
            "\n",
            "JURY: \n",
            "Score: 2  \n",
            "Rationale: You correctly identified the answer as (C) and your response hints at the dangerous nature of the activity, which is related to the assumption of risk doctrine. However, you did not explicitly mention or explain the assumption of risk doctrine itself (or discuss how the waiver factors in), and your justification is quite minimal.\n",
            "\n",
            "JUDGE: \n",
            "Story: As Sarah continues to recover from her injuries, she decides to consult a lawyer to explore any potential legal options she might have regarding the bungee jumping incident. Sarah recalls the exact wording and details of the waiver she signed before the jump and seeks legal advice on the enforceability and implications of such a document in her situation.\n",
            "\n",
            "Question: In Sarah's case, what legal concept will the lawyer likely focus on when assessing the enforceability of the waiver she signed before bungee jumping?\n",
            "(A) Prima facie.\n",
            "(B) Unconscionability.\n",
            "(C) Executed contract.\n",
            "(D) Novation.\n",
            "\n",
            "ATTORNEY (Enter response): \n",
            "The correct answer is D\n",
            "\n",
            "JURY: \n",
            "Score: 0  \n",
            "Rationale: You chose answer D, which is incorrect; the correct answer is B. Additionally, your response does not mention the key concept of unconscionability, nor does it provide an explanation regarding how unconscionability applies to assessing the waiver's enforceability.\n",
            "\n",
            "JUDGE: \n",
            "Story: Sarah, feeling determined to explore her legal options further, meets with her lawyer to delve into the details of the waiver she signed before the bungee jumping incident. As they review the document, Sarah's lawyer explains the implications of the waiver and its enforceability in light of Sarah's injuries resulting from the bungee cord snapping.\n",
            "\n",
            "Question: What legal concept might Sarah's lawyer consider when evaluating the waiver Sarah signed before bungee jumping, particularly in the context of her injuries following the bungee cord failure?\n",
            "(A) Parol evidence rule.\n",
            "(B) Exculpatory clause.\n",
            "(C) Rescission.\n",
            "(D) Quantum meruit.\n",
            "\n",
            "ATTORNEY (Enter response): \n",
            "C\n",
            "\n",
            "JURY: \n",
            "Score: 0  \n",
            "Rationale: The correct answer was (B) Exculpatory clause, but the response provided was (C) Rescission. The user did not mention or discuss the key legal concept of an exculpatory clause, nor provided a justification that would align with the legal implications of the waiver. Consequently, the answer receives 0 points.\n",
            "\n",
            "JUDGE: \n",
            "Story: Sarah, during her meeting with her lawyer, meticulously reviews the waiver she signed before the bungee jumping incident. As they analyze the document together, Sarah's lawyer points out specific clauses that outline the risks associated with the activity and the company's disclaimer of liability for any injuries sustained during the jump. Despite the waiver's detailed provisions, Sarah is eager to understand if there are any exceptions that could potentially hold the bungee jumping company accountable for her injuries.\n",
            "\n",
            "Question: In Sarah's situation, what legal concept might her lawyer explore to determine if there are exceptions that could override the waiver's disclaimer of liability in the bungee jumping incident?\n",
            "(A) Public policy.\n",
            "(B) Consideration.\n",
            "(C) Impossibility.\n",
            "(D) Adhesion contract.\n",
            "\n",
            "ATTORNEY (Enter response): \n",
            "The correct answer is D because of the adhesion contracts relevance\n",
            "\n",
            "JURY: \n",
            "Score: 0  \n",
            "Rationale: The correct answer was (A) Public policy, but the response provided (D) Adhesion contract, so the answer letter is incorrect. Additionally, the response did not mention or explain the proper legal doctrine of public policy that might override the waiver's disclaimer of liability. The justification offered is based on adhesion contracts, which is not the relevant legal concept for this scenario, and therefore does not merit any points for mentioning the correct doctrine or providing an acceptable justification.\n",
            "\n",
            "JUDGE: \n",
            "Story: Sarah, after exploring her legal options with her lawyer regarding the bungee jumping incident, decides to pursue legal action against the bungee jumping company despite the waiver she signed. Sarah believes that there are exceptions that could potentially override the company's disclaimer of liability, and she is determined to seek justice for the injuries she sustained. As the case progresses, Sarah's lawyer prepares to argue their position in court, emphasizing the circumstances surrounding the bungee cord failure and the waiver's enforceability.\n",
            "\n",
            "Question: In Sarah's upcoming legal battle against the bungee jumping company, what legal principle could her lawyer invoke to challenge the enforceability of the waiver and hold the company accountable for her injuries?\n",
            "(A) Frustration of purpose.\n",
            "(B) Waiver of liability.\n",
            "(C) Unilateral mistake.\n",
            "(D) Gross negligence.\n",
            "\n",
            "ATTORNEY (Enter response): \n",
            "D\n",
            "\n",
            "JURY: \n",
            "Score: 4  \n",
            "Rationale: You correctly identified the answer as (D) Gross negligence. You mentioned gross negligence directly, which is the legal doctrine at play, and your justification clearly explains that it involves a severe deviation from the accepted standard of care. This explanation is solid and complete, fully addressing Sarah's legal scenario.\n",
            "CASE CLOSED.\n",
            "\n",
            "Results:\n",
            "F \n",
            "Grade: 37.5%\n",
            "You've Lost.\n"
          ]
        }
      ],
      "source": [
        "## test with scoring\n",
        "def get_score(model_response):\n",
        "  score = model_response.split(\"\\n\")[0]\n",
        "  score = int(score.split(\" \")[1])\n",
        "  return score\n",
        "\n",
        "def get_letter_grade(grade):\n",
        "  letter_grade = \"\"\n",
        "\n",
        "  if 100 >= grade >= 90:\n",
        "    letter_grade = \"A\"\n",
        "  elif 89 >= grade >= 80:\n",
        "    letter_grade = \"B\"\n",
        "  elif 79 >= grade >= 70:\n",
        "    letter_grade = \"C\"\n",
        "  elif 69 >= grade >= 60:\n",
        "    letter_grade = \"D\"\n",
        "  elif 59 >= grade >= 0:\n",
        "    letter_grade = \"F\"\n",
        "  else:\n",
        "    letter_grade = \"error\"\n",
        "\n",
        "  return letter_grade\n",
        "\n",
        "questions_to_win = 6\n",
        "story_so_far = None\n",
        "user_response = \"\"\n",
        "win = False\n",
        "stop = False\n",
        "total_points = 0\n",
        "count = 0\n",
        "\n",
        "# Run game (use -1 to quit for now)\n",
        "while not stop and user_response != \"-1\":\n",
        "  if count == 0:\n",
        "    # Start case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(initial_prompt(), \"gpt-3.5-turbo-0125\")\n",
        "    print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "    story_so_far = question.split(\"/\")[0]\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "    print(model_response)\n",
        "    total_points += get_score(model_response)\n",
        "\n",
        "  elif count == questions_to_win - 1:\n",
        "    # End case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(end_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "    print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "    print(model_response)\n",
        "    total_points += get_score(model_response)\n",
        "    stop = True\n",
        "\n",
        "  else:\n",
        "    # Middle case\n",
        "    print(\"\\nJUDGE: \")\n",
        "    question = generate_response(middle_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "    print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "    story_so_far += question.split(\"/\")[0]\n",
        "    print(\"\\nATTORNEY (Enter response): \")\n",
        "    user_response = input()\n",
        "    print(\"\\nJURY: \")\n",
        "    model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "    print(model_response)\n",
        "    total_points += get_score(model_response)\n",
        "\n",
        "  count += 1\n",
        "print(\"CASE CLOSED.\\n\")\n",
        "\n",
        "print(\"Results:\")\n",
        "grade = (total_points / (questions_to_win * 4)) * 100\n",
        "letter_grade = get_letter_grade(grade)\n",
        "\n",
        "if grade >= 70:\n",
        "  win = True\n",
        "\n",
        "if win:\n",
        "  print(letter_grade, \"\\nGrade: \" + str(grade) + \"%\\nYou've Won!\")\n",
        "else:\n",
        "  print(letter_grade, \"\\nGrade: \" + str(grade) + \"%\\nYou've Lost.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST8FLBvpWRFa"
      },
      "source": [
        "# **Demo Game With Scoring and Hints**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2-CMW7F_gCQ"
      },
      "outputs": [],
      "source": [
        "o3mini_hprompt =  (\n",
        "    \"You are a legal evaluator. Given a question, the correct response, a previous hint already given\"\n",
        "    \"return a hint (no need to preface with anything, just the text) to help get the correct answer.\"\n",
        "    \"The hint should be concise and relevant, and should be more helpful than the previous hint\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktXtM5hf_c36"
      },
      "outputs": [],
      "source": [
        "## Scoring with hints\n",
        "def get_score(model_response):\n",
        "  score = model_response.split(\"\\n\")[0]\n",
        "  score = int(score.split(\" \")[1])\n",
        "  return score\n",
        "\n",
        "def get_letter_grade(grade):\n",
        "  letter_grade = \"\"\n",
        "\n",
        "  if 100 >= grade >= 90:\n",
        "    letter_grade = \"A\"\n",
        "  elif 89 >= grade >= 80:\n",
        "    letter_grade = \"B\"\n",
        "  elif 79 >= grade >= 70:\n",
        "    letter_grade = \"C\"\n",
        "  elif 69 >= grade >= 60:\n",
        "    letter_grade = \"D\"\n",
        "  elif 59 >= grade >= 0:\n",
        "    letter_grade = \"F\"\n",
        "  else:\n",
        "    letter_grade = \"error\"\n",
        "\n",
        "  return letter_grade\n",
        "\n",
        "\n",
        "def demo():\n",
        "\n",
        "  questions_to_win = 6\n",
        "  story_so_far = None\n",
        "  user_response = \"\"\n",
        "  win = False\n",
        "  stop = False\n",
        "  total_points = 0\n",
        "  count = 0\n",
        "  amount_hint = 0\n",
        "  hint = \"\"\n",
        "\n",
        "  # Run game (use -1 to quit for now)\n",
        "  while not stop and user_response != \"-1\":\n",
        "    if count == 0:\n",
        "      # Start case\n",
        "      amount_hint = 0\n",
        "      print(\"\\nJUDGE: \")\n",
        "      question = generate_response(initial_prompt(), \"gpt-3.5-turbo-0125\")\n",
        "      print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "      story_so_far = question.split(\"/\")[0]\n",
        "\n",
        "      while True:\n",
        "        print(\"\\nATTORNEY (Enter response): \")\n",
        "        user_response = input()\n",
        "        if (user_response == \"hint\" or user_response == \"Hint\") and amount_hint <= 2:\n",
        "          hint = evaluate_response(question, hint, o3mini_hprompt, \"o3-mini\")\n",
        "          print(\"\\nHINT:\")\n",
        "          print(hint)\n",
        "          amount_hint += 1\n",
        "        elif (user_response == \"hint\" or user_response == \"Hint\") and amount_hint > 2:\n",
        "          print(\"\\nYOU HAVE USED UP THE MAXIMUM AMOUNT OF HINTS (3):\")\n",
        "        else:\n",
        "          break\n",
        "\n",
        "      print(\"\\nJURY: \")\n",
        "      model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "      print(model_response)\n",
        "      total_points += get_score(model_response)\n",
        "\n",
        "    elif count == questions_to_win - 1:\n",
        "      # End case\n",
        "      amount_hint = 0\n",
        "      print(\"\\nJUDGE: \")\n",
        "      question = generate_response(end_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "      print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "\n",
        "      while True:\n",
        "        print(\"\\nATTORNEY (Enter response): \")\n",
        "        user_response = input()\n",
        "        if (user_response == \"hint\" or user_response == \"Hint\") and amount_hint <= 2:\n",
        "          hint = evaluate_response(question, hint, o3mini_hprompt, \"o3-mini\")\n",
        "          print(\"\\nHINT:\")\n",
        "          print(hint)\n",
        "          amount_hint += 1\n",
        "        elif (user_response == \"hint\" or user_response == \"Hint\") and amount_hint > 2:\n",
        "          print(\"\\nYOU HAVE USED UP THE MAXIMUM AMOUNT OF HINTS (3):\")\n",
        "        else:\n",
        "          break\n",
        "\n",
        "      print(\"\\nJURY: \")\n",
        "      model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "      print(model_response)\n",
        "      total_points += get_score(model_response)\n",
        "      stop = True\n",
        "\n",
        "    else:\n",
        "      # Middle case\n",
        "      amount_hint = 0\n",
        "      print(\"\\nJUDGE: \")\n",
        "      question = generate_response(middle_prompt(story_so_far), \"gpt-3.5-turbo-0125\")\n",
        "      print(question.split(\"\\n\\n\")[0] + \"\\n\\n\" + question.split(\"\\n\\n\")[1])\n",
        "      story_so_far += question.split(\"/\")[0]\n",
        "\n",
        "      while True:\n",
        "        print(\"\\nATTORNEY (Enter response): \")\n",
        "        user_response = input()\n",
        "        if (user_response == \"hint\" or user_response == \"Hint\") and amount_hint <= 2:\n",
        "          hint = evaluate_response(question, hint, o3mini_hprompt, \"o3-mini\")\n",
        "          print(\"\\nHINT:\")\n",
        "          print(hint)\n",
        "          amount_hint += 1\n",
        "        elif (user_response == \"hint\" or user_response == \"Hint\") and amount_hint > 2:\n",
        "          print(\"\\nYOU HAVE USED UP THE MAXIMUM AMOUNT OF HINTS (3):\")\n",
        "        else:\n",
        "          break\n",
        "      print(\"\\nJURY: \")\n",
        "\n",
        "      model_response = evaluate_response(question, user_response, o3mini_prompt, \"o3-mini\")\n",
        "      print(model_response)\n",
        "      total_points += get_score(model_response)\n",
        "\n",
        "    count += 1\n",
        "  print(\"CASE CLOSED.\\n\")\n",
        "\n",
        "  print(\"Results:\")\n",
        "  grade = (total_points / (questions_to_win * 4)) * 100\n",
        "  letter_grade = get_letter_grade(grade)\n",
        "\n",
        "  if grade >= 70:\n",
        "    win = True\n",
        "\n",
        "  if win:\n",
        "    print(letter_grade, \"\\nGrade: \" + str(grade) + \"%\\nYou've Won!\")\n",
        "  else:\n",
        "    print(letter_grade, \"\\nGrade: \" + str(grade) + \"%\\nYou've Lost.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dABC2EKAdIAg",
        "outputId": "4802ab41-3dfb-4cd7-86da-9a15f1819a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "JUDGE: \n",
            "Story: In the bustling city of London, a major financial scandal has come to light, implicating several high-profile individuals in fraudulent activities. The regulatory authority, known as the Financial Conduct Authority (FCA), has been conducting a thorough investigation into the matter and is preparing an official report detailing its findings. Among those under scrutiny is a well-known investment banker named Alex Thompson.\n",
            "\n",
            "Question: As the FCA finalizes its report on the financial scandal, what legal practice might Alex Thompson invoke to respond to any criticisms made against him before the report is published?\n",
            "(A) Miranda Rights\n",
            "(B) Solicitor-Client Privilege\n",
            "(C) Maxwellisation\n",
            "(D) Habeas Corpus\n",
            "\n",
            "ATTORNEY (Enter response): \n",
            "hint\n",
            "\n",
            "HINT:\n",
            "Think about which term is uniquely related to the right of someone under scrutiny to respond publicly to forthcoming negative information before it’s made official—it's not about criminal rights, lawyer confidentiality, or detention protection.\n",
            "\n",
            "ATTORNEY (Enter response): \n",
            "C\n",
            "\n",
            "JURY: \n",
            "Score: 0\n",
            "\n",
            "Rationale: You selected answer (C), which is incorrect. The correct choice is (B) Solicitor-Client Privilege—a recognized legal doctrine that protects confidential communications between a client and their lawyer. Additionally, you did not mention any legal concept or provide any justification supporting your answer.\n",
            "\n",
            "JUDGE: \n",
            "Story: In the midst of the ongoing investigation by the Financial Conduct Authority (FCA) into the financial scandal, Alex Thompson, the investment banker, is becoming increasingly concerned about the potential criticisms that may arise against him in the forthcoming report. Worried about safeguarding his reputation and addressing any allegations effectively, he decides to seek legal advice to prepare for the repercussions.\n",
            "\n",
            "Question: To ensure that Alex Thompson has a fair opportunity to respond to any criticisms made against him before the FCA publishes its report, which legal practice might be invoked in this scenario?\n",
            "(A) Due Process\n",
            "(B) Ex Post Facto Law\n",
            "(C) Good Samaritan Law\n",
            "(D) Res Ipsa Loquitur\n",
            "\n",
            "ATTORNEY (Enter response): \n",
            "-1\n",
            "\n",
            "JURY: \n",
            "Score: 0  \n",
            "Rationale: The correct answer was (A) Due Process, and the user did not provide that answer. Additionally, no relevant legal doctrines or concepts were mentioned, and there was no proper justification provided.\n",
            "CASE CLOSED.\n",
            "\n",
            "Results:\n",
            "F \n",
            "Grade: 0.0%\n",
            "You've Lost.\n"
          ]
        }
      ],
      "source": [
        "# play the game\n",
        "demo()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
